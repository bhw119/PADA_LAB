{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old experiment version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\administrator\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (2.5.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (4.45.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (1.2.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from peft) (0.27.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (4.10.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from transformers->peft) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\administrator\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "from peft import PeftModel, PeftConfig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, base_model_path=\"nlptown/bert-base-multilingual-uncased-sentiment\", \n",
    "                 lora_model_path=None):\n",
    "        # GPU 사용 여부 확인\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load base model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(base_model_path)\n",
    "        \n",
    "        if lora_model_path:\n",
    "            # Load and merge LoRA weights\n",
    "            config = PeftConfig.from_pretrained(lora_model_path)\n",
    "            peft_model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "            # Merge LoRA weights with base model\n",
    "            merged_model = peft_model.merge_and_unload()\n",
    "            \n",
    "            self.classifier = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=merged_model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                top_k=None,\n",
    "                device=device\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=base_model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                top_k=None,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "    def get_sentiments(self, text):\n",
    "        \"\"\"연속형과 이산형 점수 모두 반환\"\"\"\n",
    "        scores = self.classifier(text)[0]\n",
    "\n",
    "        # 연속형 점수 계산 (가중 평균)\n",
    "        weighted_score = sum(float(score['label'][0]) * score['score'] for score in scores)\n",
    "        continuous_score = round(weighted_score, 3)\n",
    "\n",
    "        # 이산형 점수 계산 (가장 높은 확률의 별점)\n",
    "        discrete_label = max(scores, key=lambda x: x['score'])['label']\n",
    "        discrete_score = int(discrete_label[0])\n",
    "        discrete_confidence = round(max(scores, key=lambda x: x['score'])['score'], 3)\n",
    "\n",
    "        return continuous_score, discrete_score, discrete_confidence\n",
    "\n",
    "    def analyze_long_text(self, text, max_tokens=450):\n",
    "    \n",
    "        # 원본 텍스트의 토큰 수 확인\n",
    "        original_tokens = self.tokenizer.encode(text)\n",
    "        was_split = len(original_tokens) > max_tokens\n",
    "\n",
    "        if not was_split:\n",
    "            # 토큰 수가 max_tokens 이하면 그대로 처리\n",
    "            continuous_score, discrete_score, discrete_conf = self.get_sentiments(text)\n",
    "            return continuous_score, discrete_score, discrete_conf, was_split\n",
    "\n",
    "        # 토큰을 청크로 나누기 (문장 단위로 나누기 추가)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        # 문장 단위로 텍스트 분할\n",
    "        sentences = text.replace('?', '.').replace('!', '.').split('.')\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip() + '.'\n",
    "            sentence_tokens = self.tokenizer.encode(sentence)\n",
    "            \n",
    "            if current_length + len(sentence_tokens) <= max_tokens:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += len(sentence_tokens)\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = len(sentence_tokens)\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "\n",
    "        # 각 청크 분석\n",
    "        continuous_scores = []\n",
    "        discrete_scores = []\n",
    "        confidence_scores = []\n",
    "        chunk_weights = []  # 청크 길이에 기반한 가중치\n",
    "\n",
    "        for chunk in chunks:\n",
    "            try:\n",
    "                chunk_tokens = len(self.tokenizer.encode(chunk))\n",
    "                weight = chunk_tokens / len(original_tokens)\n",
    "                \n",
    "                cont_score, disc_score, conf = self.get_sentiments(chunk)\n",
    "                \n",
    "                continuous_scores.append(cont_score)\n",
    "                discrete_scores.append(disc_score)\n",
    "                confidence_scores.append(conf)\n",
    "                chunk_weights.append(weight)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"청크 처리 중 오류 발생: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if not continuous_scores:\n",
    "            raise Exception(\"모든 청크 처리 실패\")\n",
    "\n",
    "        # 가중 평균으로 최종 점수 계산\n",
    "        avg_continuous = round(\n",
    "            sum(score * weight for score, weight in zip(continuous_scores, chunk_weights)), 3\n",
    "        )\n",
    "        \n",
    "        # 가중치가 적용된 이산형 점수 계산\n",
    "        weighted_discrete_scores = []\n",
    "        for score, weight in zip(discrete_scores, chunk_weights):\n",
    "            weighted_discrete_scores.extend([score] * int(weight * 100))\n",
    "        most_common_rating = Counter(weighted_discrete_scores).most_common(1)[0][0]\n",
    "        \n",
    "        # 평균 confidence\n",
    "        avg_confidence = round(\n",
    "            sum(conf * weight for conf, weight in zip(confidence_scores, chunk_weights)), 3\n",
    "        )\n",
    "\n",
    "        return avg_continuous, most_common_rating, avg_confidence, was_split\n",
    "\n",
    "    def read_file(self, file_path):\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        if file_extension == '.csv':\n",
    "            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    return pd.read_csv(file_path, encoding=encoding)\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {encoding} encoding: {str(e)}\")\n",
    "                    continue\n",
    "            raise ValueError(f\"Could not read file with any of the encodings: {encodings}\")\n",
    "        elif file_extension in ['.xlsx', '.xls']:\n",
    "            return pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "\n",
    "    def save_file(self, df, file_path = None):\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        if file_extension == '.csv':\n",
    "            df.to_csv(file_path, index=False)\n",
    "        elif file_extension in ['.xlsx', '.xls']:\n",
    "            df.to_excel(file_path, index=False)\n",
    "\n",
    "    def process_file(self, file_path, text_column, save_path=None, batch_size=1000):\n",
    "        df = self.read_file(file_path)\n",
    "        if df[\"Rating\"].value_counts().shape[0] > 5:\n",
    "            def convert_to_categories(rating):\n",
    "                if rating in [1, 2]:\n",
    "                    return 1\n",
    "                elif rating in [3, 4]:\n",
    "                    return 2\n",
    "                elif rating in [5, 6]:\n",
    "                    return 3\n",
    "                elif rating in [7, 8]:\n",
    "                    return 4\n",
    "                else:\n",
    "                    return 5\n",
    "\n",
    "            # 등급 변환 적용\n",
    "            df['Rating'] = df['Rating'].apply(convert_to_categories)\n",
    "        total_rows = len(df)\n",
    "        num_batches = (total_rows + batch_size - 1) // batch_size\n",
    "\n",
    "        # 결과 컬럼만 생성\n",
    "        df['sentiment_score_continuous'] = None\n",
    "        df['sentiment_score_discrete'] = None\n",
    "\n",
    "        print(f\"\\nCUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "        print(f\"현재 장치: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "        print(f\"총 {total_rows}개 데이터 처리 시작\\n\")\n",
    "\n",
    "        for i in tqdm(range(0, total_rows, batch_size), desc=\"Processing\"):\n",
    "            batch_end = min(i + batch_size, total_rows)\n",
    "\n",
    "            for idx in range(i, batch_end):\n",
    "                try:\n",
    "                    text = str(df.loc[idx, text_column])\n",
    "                    if pd.isna(text) or text.strip() == '':\n",
    "                        df.loc[idx, 'sentiment_score_continuous'] = None\n",
    "                        df.loc[idx, 'sentiment_score_discrete'] = None\n",
    "                    else:\n",
    "                        cont_score, disc_score, _, _ = self.analyze_long_text(text)\n",
    "                        df.loc[idx, 'sentiment_score_continuous'] = cont_score\n",
    "                        df.loc[idx, 'sentiment_score_discrete'] = disc_score\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing row {idx}: {str(e)}\")\n",
    "                    df.loc[idx, 'sentiment_score_continuous'] = None\n",
    "                    df.loc[idx, 'sentiment_score_discrete'] = None\n",
    "\n",
    "            save_path = save_path or file_path\n",
    "            self.save_file(df, save_path)\n",
    "            print(f\"\\n배치 {i // batch_size + 1}/{num_batches} 처리 완료\")\n",
    "\n",
    "        print(\"\\n전체 처리 완료\")\n",
    "        print(\"\\n별점 분포:\")\n",
    "        print(df['sentiment_score_discrete'].value_counts().sort_index())\n",
    "        \n",
    "\n",
    "        return df\n",
    "    def EDA_data(self, data):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"original data distribution\")\n",
    "        plt.hist(data[\"Rating\"],color = \"blue\", bins = 5)\n",
    "        \n",
    "        ax1 = plt.subplot(1,2,2)\n",
    "        plt.title(\"bert sentiment distribution\")\n",
    "        ax1.hist(data[\"sentiment_score_discrete\"],color = 'green', alpha = 0.6, bins = 5)\n",
    "        ax2 = ax1.twinx()\n",
    "        sns.kdeplot(data = data[\"sentiment_score_continuous\"], color = 'red', ax = ax2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # LoRA 모델을 사용하는 경우\n",
    "    analyzer = SentimentAnalyzer(\n",
    "    base_model_path=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    lora_model_path=None  # LoRA 모델 경로를 None으로 설정\n",
    ")\n",
    "\n",
    "    df = analyzer.process_file(\n",
    "        file_path=r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\calculated\\final\\hotel_reviews_with_topics_and_depth_breadth.csv',\n",
    "        text_column='Review_Text',\n",
    "        save_path=r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\calculated\\sampled_hotel_BERT_test.csv',\n",
    "        batch_size=1500\n",
    "    )\n",
    "    analyzer.EDA_data(df)\n",
    "\n",
    "    # 리소스 해제\n",
    "    del analyzer.classifier\n",
    "    del analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Rating','Review_Text','sentiment_score_continuous', 'sentiment_score_discrete']\n",
    "df_select = df[columns].copy()\n",
    "df_select[\"deviation\"] = df_select[\"Rating\"] - df_select[\"sentiment_score_continuous\"]\n",
    "df_select['abs_deviation'] = abs(df_select['deviation'])\n",
    "df_select = df_select.sort_values('abs_deviation', ascending=False)\n",
    "df_select.head(20)\n",
    "df_select.to_csv(r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\sentimnet\\Rating_sample.csv ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본 별점 분포\n",
    "\n",
    "별점 분포:\n",
    "sentiment_score_discrete\n",
    "1     31\n",
    "2     37\n",
    "3    111\n",
    "4    448\n",
    "5    346\n",
    "Name: count, dtype: int64\n",
    "\n",
    "### 기본 모델 별점 분포\n",
    "\n",
    "별점 분포:\n",
    "sentiment_score_discrete\n",
    "1     41\n",
    "2    105\n",
    "3    172\n",
    "4    401\n",
    "5    254\n",
    "Name: count, dtype: int64\n",
    "\n",
    "### LoRA 파인튜닝 모델\n",
    "\n",
    "별점 분포:\n",
    "sentiment_score_discrete\n",
    "1     64\n",
    "2      2\n",
    "3     19\n",
    "4    637\n",
    "5    251\n",
    "Name: count, dtype: int64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW version of sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194b5cffb280424ab423ec1094619972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff68aedd8644f68a758692c2f2119d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19c887ee8714320b0fb770af6f4c5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/380 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8f785d52244896900caefe09c9f02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b305a3cfbb144a399f008330bd435930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b231d0983c9440a5ae9504328e24b94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5254c25b9e5f4aabb155959c1c29ee84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "\n",
    "# 기본 모델 ID\n",
    "model_id = \"SamLowe/roberta-base-go_emotions\"\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import re\n",
    "\n",
    "def calculate_va_scores(emotion_results, va_df):\n",
    "    \"\"\"\n",
    "    감정 분석 결과를 바탕으로 VA 점수를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        emotion_results (list): 감정 분석 결과 리스트\n",
    "        va_df (pd.DataFrame): VA 점수가 저장된 데이터프레임\n",
    "\n",
    "    Returns:\n",
    "        tuple: (valence 점수, arousal 점수)\n",
    "    \"\"\"\n",
    "    if emotion_results is None:\n",
    "        return None, None\n",
    "\n",
    "    weighted_valence = 0\n",
    "    weighted_arousal = 0\n",
    "    total_weight = 0\n",
    "\n",
    "    for emotion in emotion_results:\n",
    "        label = emotion['label']\n",
    "        score = emotion['score']\n",
    "\n",
    "        # VA 값 찾기\n",
    "        va_row = va_df[va_df['emotion'] == label]\n",
    "        if not va_row.empty:\n",
    "            if not pd.isna(va_row['valence'].iloc[0]) and not pd.isna(va_row['arousal'].iloc[0]):\n",
    "                weighted_valence += va_row['valence'].iloc[0] * score\n",
    "                weighted_arousal += va_row['arousal'].iloc[0] * score\n",
    "                total_weight += score\n",
    "\n",
    "    if total_weight > 0:\n",
    "        return weighted_valence / total_weight, weighted_arousal / total_weight\n",
    "    return None, None\n",
    "\n",
    "def calculate_va_scores(emotion_results, va_df):\n",
    "    \"\"\"\n",
    "    감정 분석 결과를 바탕으로 VA 점수를 계산합니다.\n",
    "    \"\"\"\n",
    "    if emotion_results is None:\n",
    "        return None, None\n",
    "\n",
    "    weighted_valence = 0\n",
    "    weighted_arousal = 0\n",
    "    total_weight = 0\n",
    "\n",
    "    for emotion in emotion_results:\n",
    "        label = emotion['label']\n",
    "        score = emotion['score']\n",
    "\n",
    "        va_row = va_df[va_df['emotion'] == label]\n",
    "        if not va_row.empty:\n",
    "            if not pd.isna(va_row['valence'].iloc[0]) and not pd.isna(va_row['arousal'].iloc[0]):\n",
    "                weighted_valence += va_row['valence'].iloc[0] * score\n",
    "                weighted_arousal += va_row['arousal'].iloc[0] * score\n",
    "                total_weight += score\n",
    "\n",
    "    if total_weight > 0:\n",
    "        return weighted_valence / total_weight, weighted_arousal / total_weight\n",
    "    return None, None\n",
    "\n",
    "def analyze_emotions_df(df, text_column, model, tokenizer, va_scores_file, max_length=512, batch_size=8):\n",
    "    \"\"\"\n",
    "    데이터프레임의 텍스트를 분석하여 VA 값을 계산합니다.\n",
    "    \"\"\"\n",
    "    va_df = pd.read_csv(va_scores_file)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    classifier = pipeline(\n",
    "        task=\"text-classification\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        top_k=None,\n",
    "        function_to_apply=\"sigmoid\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    va_scores = []\n",
    "\n",
    "    for text in df[text_column]:\n",
    "        try:\n",
    "            if pd.isna(text) or text == \"\":\n",
    "                va_scores.append((None, None))\n",
    "                continue\n",
    "\n",
    "            text = str(text).strip()\n",
    "            encoded_length = len(tokenizer.encode(text))\n",
    "\n",
    "            if encoded_length > max_length:\n",
    "                chunks = split_text(text, max_length, tokenizer)\n",
    "                chunk_results = classifier(chunks)\n",
    "\n",
    "                # 청크들의 평균 점수 계산\n",
    "                all_labels = set()\n",
    "                for chunk in chunk_results:\n",
    "                    for emotion in chunk:\n",
    "                        all_labels.add(emotion['label'])\n",
    "\n",
    "                combined_scores = {label: [] for label in all_labels}\n",
    "                for chunk in chunk_results:\n",
    "                    for emotion in chunk:\n",
    "                        combined_scores[emotion['label']].append(emotion['score'])\n",
    "\n",
    "                final_results = [\n",
    "                    {\n",
    "                        'label': label,\n",
    "                        'score': np.mean(scores)\n",
    "                    }\n",
    "                    for label, scores in combined_scores.items()\n",
    "                ]\n",
    "\n",
    "                final_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "                valence, arousal = calculate_va_scores(final_results, va_df)\n",
    "            else:\n",
    "                results = classifier(text)\n",
    "                valence, arousal = calculate_va_scores(results[0], va_df)\n",
    "\n",
    "            va_scores.append((valence, arousal))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {str(e)}\")\n",
    "            print(f\"Text preview: {text[:100]}...\")\n",
    "            va_scores.append((None, None))\n",
    "\n",
    "    # VA 점수만 추가\n",
    "    df['valence'] = [va[0] for va in va_scores]\n",
    "    df['arousal'] = [va[1] for va in va_scores]\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_text(text: str, max_length: int, tokenizer) -> List[str]:\n",
    "    encoded = tokenizer.encode(text)\n",
    "    if len(encoded) <= max_length:\n",
    "        return [text]\n",
    "\n",
    "    sentences = re.split('(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = len(tokenizer.encode(sentence))\n",
    "\n",
    "        if current_length + sentence_tokens <= max_length:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_tokens\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임 준비\n",
    "df = pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\sentimnet\\emotion_va_scores.csv')\n",
    "\n",
    "# 감정 분석 실행\n",
    "results_df = analyze_emotions_df(\n",
    "    df=df,\n",
    "    text_column='Review_Text',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    va_scores_file='/content/drive/MyDrive/sentiment_data/emotion_va_scores.csv'  # VA 점수가 있는 CSV 파일\n",
    ")\n",
    "# -1 ~ 1 을 0 ~ 1로 변환\n",
    "def scale_to_one(x):\n",
    "    return (x + 1) / 2\n",
    "def scale_to_five(x):\n",
    "    # x * 4: 0~1의 범위를 0~4로 변환\n",
    "    # + 1: 0~4의 범위를 1~5로 변환\n",
    "    return (x * 4) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
