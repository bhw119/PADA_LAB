{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 데이터 로드\n",
    "df_amazon = pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\raw\\amazon.csv', encoding='utf-8')\n",
    "df_audible = pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\raw\\audible.csv', encoding='utf-8')\n",
    "df_coursera = pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\raw\\coursera.csv', encoding='cp949')\n",
    "df_hotel = pd.read_csv(r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\raw\\hotel.csv', encoding='utf-8')\n",
    "\n",
    "sampled_dfs = []\n",
    "n = 100\n",
    "\n",
    "for i in range(1, 6):\n",
    "    # Amazon\n",
    "    df_1 = df_amazon[df_amazon['Rating'] == i].sample(n)[['Rating', 'Review_Text']]\n",
    "    \n",
    "    # Audible\n",
    "    df_2 = df_audible[df_audible['Rating'] == i].sample(n)[['Rating', 'Review_Text']]  # 컬럼명 확인 필요\n",
    "    \n",
    "    # Coursera\n",
    "    df_3 = df_coursera[df_coursera['Rating'] == i].sample(n)[['Rating', 'Review_Text']]  # 컬럼명 확인 필요\n",
    "    \n",
    "    # Hotel\n",
    "    df_4 = df_hotel[(df_hotel['Rating'] <= 2*i) & (df_hotel['Rating'] > 2*(i-1))].sample(n)[['Rating', 'Review_Text']]\n",
    "    \n",
    "    sampled_dfs.extend([df_1, df_2, df_3, df_4])\n",
    "\n",
    "# 모든 샘플 합치기\n",
    "df_sample = pd.concat(sampled_dfs).reset_index(drop=True)\n",
    "df_sample.to_csv(r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\sentimnet\\2000_sample.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "# CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing text 1/40\n",
      "\n",
      "Processing text 2/40\n",
      "\n",
      "Processing text 3/40\n",
      "\n",
      "Processing text 4/40\n",
      "\n",
      "Processing text 5/40\n",
      "\n",
      "Processing text 6/40\n",
      "\n",
      "Processing text 7/40\n",
      "\n",
      "Processing text 8/40\n",
      "\n",
      "Processing text 9/40\n",
      "\n",
      "Processing text 10/40\n",
      "\n",
      "Processing text 11/40\n",
      "\n",
      "Processing text 12/40\n",
      "\n",
      "Processing text 13/40\n",
      "\n",
      "Processing text 14/40\n",
      "\n",
      "Processing text 15/40\n",
      "\n",
      "Processing text 16/40\n",
      "\n",
      "Processing text 17/40\n",
      "\n",
      "Processing text 18/40\n",
      "\n",
      "Processing text 19/40\n",
      "\n",
      "Processing text 20/40\n",
      "\n",
      "Processing text 21/40\n",
      "\n",
      "Processing text 22/40\n",
      "\n",
      "Processing text 23/40\n",
      "\n",
      "Processing text 24/40\n",
      "\n",
      "Processing text 25/40\n",
      "\n",
      "Processing text 26/40\n",
      "\n",
      "Processing text 27/40\n",
      "\n",
      "Processing text 28/40\n",
      "\n",
      "Processing text 29/40\n",
      "\n",
      "Processing text 30/40\n",
      "\n",
      "Processing text 31/40\n",
      "\n",
      "Processing text 32/40\n",
      "\n",
      "Processing text 33/40\n",
      "\n",
      "Processing text 34/40\n",
      "\n",
      "Processing text 35/40\n",
      "\n",
      "Processing text 36/40\n",
      "\n",
      "Processing text 37/40\n",
      "\n",
      "Processing text 38/40\n",
      "\n",
      "Processing text 39/40\n",
      "\n",
      "Processing text 40/40\n",
      "\n",
      "Results saved to: C:\\Users\\Administrator\\Desktop\\PADA_LAB\\sentimnet\\improved_sentiment_analysis_results.csv\n",
      "\n",
      "First few rows of the results:\n",
      "   text_id                                      original_text  rating  \\\n",
      "0        1  Are these used headsets?! They do not work! So...     1.0   \n",
      "1        2  I bought Logitech M510 Wireless Computer Mouse...     1.0   \n",
      "2        3  Did I miss something I purchased this book bec...     1.0   \n",
      "3        4  I couldnt even finish the book  The main chara...     1.0   \n",
      "4        5  The content is ok, but the grading is broken. ...     1.0   \n",
      "\n",
      "     word_1  similarity_1  valence_1  arousal_1      word_2  similarity_2  \\\n",
      "0   headset      0.615021      0.204     -0.320  headphones      0.558549   \n",
      "1     mouse      0.475608     -0.146     -0.118   mousetrap      0.370554   \n",
      "2  massacre      0.421742     -0.938      0.774   murderous      0.397774   \n",
      "3     novel      0.524619      0.404     -0.296    novelist      0.482484   \n",
      "4    graded      0.459509      0.124     -0.320  assessment      0.390208   \n",
      "\n",
      "   valence_2  ...  valence_3 arousal_3    word_4  similarity_4  valence_4  \\\n",
      "0      0.200  ...      0.062     0.138  earpiece      0.417244      0.166   \n",
      "1     -0.640  ...     -0.164    -0.020  squamous      0.299985     -0.312   \n",
      "2     -0.966  ...     -0.020     0.592     novel      0.384905      0.404   \n",
      "3     -0.320  ...      0.416    -0.500  literary      0.429328      0.438   \n",
      "4      0.500  ...      0.326    -0.196     grade      0.372195      0.396   \n",
      "\n",
      "  arousal_4     word_5  similarity_5  valence_5 arousal_5  \n",
      "0    -0.428   headband      0.365555      0.340    -0.062  \n",
      "1    -0.108      click      0.292694      0.072    -0.270  \n",
      "2    -0.296   murderer      0.378764     -0.980     0.920  \n",
      "3    -0.352  storybook      0.404720      0.708    -0.388  \n",
      "4    -0.280     grades      0.365694      0.042    -0.302  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict\n",
    "\n",
    "class ImprovedSentimentAnalyzer:\n",
    "    def __init__(self, lexicon_embeddings_path: str = 'lexicon_embeddings.csv'):\n",
    "        # Sentence Transformer 모델 로드\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # 렉시콘 데이터 로드\n",
    "        self.lexicon_data = pd.read_csv(lexicon_embeddings_path)\n",
    "        # 감정 단어들의 임베딩 생성\n",
    "        self.word_embeddings = self.model.encode(self.lexicon_data['word'].tolist())\n",
    "    \n",
    "    def find_similar_words(self, text: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"문장 임베딩과 가장 유사한 감정 단어들을 찾음\"\"\"\n",
    "        # 문장 임베딩 생성\n",
    "        sentence_embedding = self.model.encode(text)\n",
    "        \n",
    "        # 코사인 유사도 계산\n",
    "        similarities = cosine_similarity([sentence_embedding], self.word_embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        similar_words = []\n",
    "        for idx in top_indices:\n",
    "            similar_words.append({\n",
    "                'word': self.lexicon_data.iloc[idx]['word'],\n",
    "                'similarity': similarities[idx],\n",
    "                'valence': self.lexicon_data.iloc[idx]['valence'],\n",
    "                'arousal': self.lexicon_data.iloc[idx]['arousal']\n",
    "            })\n",
    "        \n",
    "        return similar_words\n",
    "\n",
    "def main():\n",
    "    # ImprovedSentimentAnalyzer 초기화\n",
    "    analyzer = ImprovedSentimentAnalyzer(lexicon_embeddings_path=r\"C:\\Users\\Administrator\\Desktop\\lexicon_embeddings_ver2.csv\")\n",
    "    \n",
    "    # 리뷰 데이터 로드\n",
    "    test_texts = df_sample['Review_Text'].tolist()\n",
    "    \n",
    "    # 결과를 저장할 리스트\n",
    "    results = []\n",
    "    \n",
    "    # 각 텍스트 분석\n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        print(f\"\\nProcessing text {i}/{len(test_texts)}\")\n",
    "        similar_words = analyzer.find_similar_words(text, top_k=5)\n",
    "        \n",
    "        # 결과 저장\n",
    "        result = {\n",
    "            'text_id': i,\n",
    "            'original_text': text,\n",
    "            'rating': df_sample.iloc[i-1]['Rating']\n",
    "        }\n",
    "        \n",
    "        # 상위 5개 단어의 정보 추가\n",
    "        for j, word in enumerate(similar_words, 1):\n",
    "            result[f'word_{j}'] = word['word']\n",
    "            result[f'similarity_{j}'] = word['similarity']\n",
    "            result[f'valence_{j}'] = word['valence']\n",
    "            result[f'arousal_{j}'] = word['arousal']\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # 결과를 데이터프레임으로 변환\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # CSV 파일로 저장\n",
    "    output_path = r'C:\\Users\\Administrator\\Desktop\\PADA_LAB\\sentimnet\\improved_sentiment_analysis_results.csv'\n",
    "    df_results.to_csv(output_path, index=False, encoding='cp949')\n",
    "    print(f\"\\nResults saved to: {output_path}\")\n",
    "    \n",
    "    # 결과 미리보기\n",
    "    print(\"\\nFirst few rows of the results:\")\n",
    "    print(df_results.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
